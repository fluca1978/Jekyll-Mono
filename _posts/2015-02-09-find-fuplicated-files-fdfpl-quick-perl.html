---
layout: post
title: 'Find Fuplicated Files (fdf.pl): a quick Perl script to the task!'
date: '2015-02-09T20:57:00.003+01:00'
author: Luca Ferrari
tags:
- perl
- programmazione
- planet-perl-ironman
modified_time: '2015-02-16T22:07:12.984+01:00'
blogger_id: tag:blogger.com,1999:blog-1836481905487384887.post-8585124015080038957
blogger_orig_url: http://fluca1978.blogspot.com/2015/02/find-fuplicated-files-fdfpl-quick-perl.html

permalink: /:year/:month/:day/:title.html
---


<h1>~</h1>


<div style="text-align: justify;">I often find my MP3 player or photo repository filled with duplicated items, and of course that results in an annoying task of cleaning up the tree structure.</div><div style="text-align: justify;">A few days ago I decided to write a very simple Perl script to get hints about such duplicated files. The script is quick and dirty, and do not aim at being a very performance one, even if it is working quite fine for me.</div><div style="text-align: justify;">Here it is:</div><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">#!/usr/bin/perl</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;"><br /></span></span><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">use strict;</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">use warnings;</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">use Digest::file qw( digest_file );</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">use File::Find;</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">use v5.10;</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;"><br /></span></span><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">die "\nPlease specify one or more directories\n" unless ( @ARGV );</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;"><br /></span></span><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">my $files = {};</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">find( { no_chdir =&gt; 1,</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">wanted   =&gt; sub {</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">push @{ $files-&gt;{ digest_file( $_, "SHA-1" ) } }, $_ if ( -f $_ );</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">}</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">}, grep { -d $_ } @ARGV );</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;"><br /></span></span><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;"><br /></span></span><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">while ( my ($sha1, $files) = each %$files ){</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">say "\n\n#Duplicated files: \n\t#rm " . join( "\n\t#rm ", @$files ) if ( @$files &gt; 1 );</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">}</span></span><br /><br /><br /><br /><div style="text-align: justify;">The idea is quite simple: I use an hash (named <i>$files</i>) indexed by the SHA-1 hash of a file. Each file with the very same hash is appended into the same hash bucket, and therefore at the end of the story each entry in the hash that has more than one file name in the bucket reveals a duplicated file.</div><div style="text-align: justify;"><br /></div><div style="text-align: justify;">As you can see, I use the file <i>File::Find</i> method with the no_chdir option to strip down the file name in the code ref, so that <i>$_</i> is the fully qualified file name. For each file, File::Find executes the code ref that tests if the entry <i>$_</i> is a file and computes the hash, placing it as key of the $files hash with the name as first value. </div><div style="text-align: justify;">The action is iterated over all the directiories supplied as script arguments, that are in turn filtered by grep to check about their directory-ness.</div><div style="text-align: justify;"><br /></div><div style="text-align: justify;">At the end, since I'm a little lazy, the script prints a list of Shell like <i>rm</i> commands to purge the duplicated files, so that I can choose the files and simply executes them.</div>